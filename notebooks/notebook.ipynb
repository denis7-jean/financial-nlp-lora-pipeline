{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mblrbOc4eRfP",
        "outputId": "908b3164-cdff-4238-c74e-9b3fab7c5d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  financial-nlp-lora-pipeline-main.zip\n",
            "41a0f3ed63aa08b0a5ee769c54d1432a72370636\n",
            "   creating: financial-nlp-lora-pipeline-main/\n",
            "  inflating: financial-nlp-lora-pipeline-main/README.md  \n",
            "   creating: financial-nlp-lora-pipeline-main/config/\n",
            "  inflating: financial-nlp-lora-pipeline-main/config/lora_config.json  \n",
            "   creating: financial-nlp-lora-pipeline-main/notebooks/\n",
            " extracting: financial-nlp-lora-pipeline-main/notebooks/.gitkeep  \n",
            "  inflating: financial-nlp-lora-pipeline-main/requirements.txt  \n",
            "   creating: financial-nlp-lora-pipeline-main/src/\n",
            " extracting: financial-nlp-lora-pipeline-main/src/__init__.py  \n",
            "  inflating: financial-nlp-lora-pipeline-main/src/data_loader.py  \n",
            "  inflating: financial-nlp-lora-pipeline-main/src/model.py  \n",
            "  inflating: financial-nlp-lora-pipeline-main/src/trainer.py  \n",
            "  inflating: financial-nlp-lora-pipeline-main/src/utils.py  \n",
            "  inflating: financial-nlp-lora-pipeline-main/train.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip financial-nlp-lora-pipeline-main.zip\n",
        "!cd financial-nlp-lora-pipeline-main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -r /content/financial-nlp-lora-pipeline-main/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599TSVVZkeGj",
        "outputId": "653184b4-b292-44cd-fa06-584d9f5fbc7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/financial-nlp-lora-pipeline-main\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1gL2acQlIZQ",
        "outputId": "24548896-8730-4ef2-d504-b820df959889"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/financial-nlp-lora-pipeline-main\n",
            "/content/financial-nlp-lora-pipeline-main\n",
            "config\tnotebooks  outputs  README.md  requirements.txt  src  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!WANDB_MODE=disabled python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuSUOS7plWxw",
        "outputId": "441d1c94-e858-4d47-9922-4f404a5c44f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 06:06:19.089157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766383579.109088    9352 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766383579.115243    9352 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766383579.130663    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766383579.130691    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766383579.130697    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766383579.130701    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 06:06:19.135260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:__main__:Loading tokenizer: ProsusAI/finbert\n",
            "INFO:__main__:Preparing data (financial_phrasebank)...\n",
            "INFO:src.data_loader:Loading financial_phrasebank (sentences_allagree) from HuggingFace hub.\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for financial_phrasebank contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/financial_phrasebank\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Map: 100% 1584/1584 [00:00<00:00, 4111.87 examples/s]\n",
            "Map: 100% 340/340 [00:00<00:00, 5118.64 examples/s]\n",
            "Map: 100% 340/340 [00:00<00:00, 6500.75 examples/s]\n",
            "INFO:src.data_loader:Prepared datasets with class weights: [2.4905660152435303, 0.5426515936851501, 1.3233082294464111]\n",
            "INFO:__main__:Detected 3 labels.\n",
            "INFO:__main__:Initializing model (LoRA=False)...\n",
            "INFO:src.model:Loading base model: ProsusAI/finbert\n",
            "INFO:src.model:Returning standard model (no LoRA).\n",
            "/content/financial-nlp-lora-pipeline-main/src/trainer.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedCELossTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "INFO:src.trainer:Initialized WeightedCELossTrainer with class weights: tensor([2.4906, 0.5427, 1.3233])\n",
            "INFO:__main__:Starting training...\n",
            "2025/12/22 06:06:33 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/12/22 06:06:33 INFO mlflow.store.db.utils: Updating database tables\n",
            "2025/12/22 06:06:33 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/22 06:06:33 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2025/12/22 06:06:33 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/22 06:06:33 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "{'loss': 1.0698, 'grad_norm': 5.216221332550049, 'learning_rate': 1.6700336700336702e-05, 'epoch': 0.51}\n",
            " 33% 99/297 [00:30<01:00,  3.28it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 2/22 [00:00<00:01, 18.65it/s]\u001b[A\n",
            " 18% 4/22 [00:00<00:01, 12.18it/s]\u001b[A\n",
            " 27% 6/22 [00:00<00:01, 10.98it/s]\u001b[A\n",
            " 36% 8/22 [00:00<00:01, 10.53it/s]\u001b[A\n",
            " 45% 10/22 [00:00<00:01, 10.18it/s]\u001b[A\n",
            " 55% 12/22 [00:01<00:00, 10.01it/s]\u001b[A\n",
            " 64% 14/22 [00:01<00:00,  9.95it/s]\u001b[A\n",
            " 73% 16/22 [00:01<00:00,  9.86it/s]\u001b[A\n",
            " 77% 17/22 [00:01<00:00,  9.85it/s]\u001b[A\n",
            " 82% 18/22 [00:01<00:00,  9.84it/s]\u001b[A\n",
            " 86% 19/22 [00:01<00:00,  9.86it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.14408642053604126, 'eval_accuracy': 0.9588235294117647, 'eval_macro_f1': 0.9349880048637812, 'eval_weighted_f1': 0.9594769791974761, 'eval_runtime': 2.1943, 'eval_samples_per_second': 154.95, 'eval_steps_per_second': 10.026, 'epoch': 1.0}\n",
            " 33% 99/297 [00:32<01:00,  3.28it/s]\n",
            "100% 22/22 [00:02<00:00,  9.91it/s]\u001b[A\n",
            "{'loss': 0.2599, 'grad_norm': 1.8466070890426636, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.01}\n",
            "{'loss': 0.1511, 'grad_norm': 6.357691764831543, 'learning_rate': 9.966329966329968e-06, 'epoch': 1.52}\n",
            " 67% 198/297 [01:31<00:32,  3.09it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 2/22 [00:00<00:01, 18.23it/s]\u001b[A\n",
            " 18% 4/22 [00:00<00:01, 11.76it/s]\u001b[A\n",
            " 27% 6/22 [00:00<00:01, 10.44it/s]\u001b[A\n",
            " 36% 8/22 [00:00<00:01,  9.94it/s]\u001b[A\n",
            " 45% 10/22 [00:00<00:01,  9.76it/s]\u001b[A\n",
            " 55% 12/22 [00:01<00:01,  9.57it/s]\u001b[A\n",
            " 59% 13/22 [00:01<00:00,  9.53it/s]\u001b[A\n",
            " 64% 14/22 [00:01<00:00,  9.44it/s]\u001b[A\n",
            " 68% 15/22 [00:01<00:00,  9.38it/s]\u001b[A\n",
            " 73% 16/22 [00:01<00:00,  9.33it/s]\u001b[A\n",
            " 77% 17/22 [00:01<00:00,  9.26it/s]\u001b[A\n",
            " 82% 18/22 [00:01<00:00,  9.25it/s]\u001b[A\n",
            " 86% 19/22 [00:01<00:00,  9.31it/s]\u001b[A\n",
            " 91% 20/22 [00:02<00:00,  9.30it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.10811909288167953, 'eval_accuracy': 0.9764705882352941, 'eval_macro_f1': 0.9669609639624631, 'eval_weighted_f1': 0.9766768576496064, 'eval_runtime': 2.3128, 'eval_samples_per_second': 147.008, 'eval_steps_per_second': 9.512, 'epoch': 2.0}\n",
            " 67% 198/297 [01:33<00:32,  3.09it/s]\n",
            "100% 22/22 [00:02<00:00,  9.30it/s]\u001b[A\n",
            "{'loss': 0.1007, 'grad_norm': 0.2304239273071289, 'learning_rate': 6.5993265993266e-06, 'epoch': 2.02}\n",
            "{'loss': 0.0522, 'grad_norm': 0.25203847885131836, 'learning_rate': 3.232323232323233e-06, 'epoch': 2.53}\n",
            "100% 297/297 [03:07<00:00,  2.99it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 2/22 [00:00<00:01, 17.26it/s]\u001b[A\n",
            " 18% 4/22 [00:00<00:01, 11.00it/s]\u001b[A\n",
            " 27% 6/22 [00:00<00:01,  9.80it/s]\u001b[A\n",
            " 36% 8/22 [00:00<00:01,  9.18it/s]\u001b[A\n",
            " 41% 9/22 [00:00<00:01,  9.17it/s]\u001b[A\n",
            " 45% 10/22 [00:01<00:01,  9.11it/s]\u001b[A\n",
            " 50% 11/22 [00:01<00:01,  9.16it/s]\u001b[A\n",
            " 55% 12/22 [00:01<00:01,  9.02it/s]\u001b[A\n",
            " 59% 13/22 [00:01<00:01,  8.92it/s]\u001b[A\n",
            " 64% 14/22 [00:01<00:00,  8.92it/s]\u001b[A\n",
            " 68% 15/22 [00:01<00:00,  8.80it/s]\u001b[A\n",
            " 73% 16/22 [00:01<00:00,  8.74it/s]\u001b[A\n",
            " 77% 17/22 [00:01<00:00,  8.74it/s]\u001b[A\n",
            " 82% 18/22 [00:01<00:00,  8.73it/s]\u001b[A\n",
            " 86% 19/22 [00:02<00:00,  8.74it/s]\u001b[A\n",
            " 91% 20/22 [00:02<00:00,  8.77it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.11232014745473862, 'eval_accuracy': 0.9764705882352941, 'eval_macro_f1': 0.9669609639624631, 'eval_weighted_f1': 0.9766768576496064, 'eval_runtime': 2.4573, 'eval_samples_per_second': 138.365, 'eval_steps_per_second': 8.953, 'epoch': 3.0}\n",
            "100% 297/297 [03:09<00:00,  2.99it/s]\n",
            "100% 22/22 [00:02<00:00,  8.75it/s]\u001b[A\n",
            "{'train_runtime': 221.679, 'train_samples_per_second': 21.436, 'train_steps_per_second': 1.34, 'train_loss': 0.2812024362962254, 'epoch': 3.0}\n",
            "100% 297/297 [03:42<00:00,  1.34it/s]\n",
            "INFO:__main__:Evaluating on test set...\n",
            "100% 22/22 [00:02<00:00,  9.73it/s]\n",
            "INFO:__main__:Test metrics: {'eval_loss': 0.11756044626235962, 'eval_accuracy': 0.9735294117647059, 'eval_macro_f1': 0.962717670038422, 'eval_weighted_f1': 0.9737899099988028, 'eval_runtime': 2.4019, 'eval_samples_per_second': 141.554, 'eval_steps_per_second': 9.159, 'epoch': 3.0}\n",
            "INFO:__main__:Saving model to ./outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!WANDB_MODE=disabled python train.py --use_lora --fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGbBXXRTzuLD",
        "outputId": "1366e5d6-a523-413a-9a95-4d81b27167d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 07:18:02.046354: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766387882.066568   27413 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766387882.072562   27413 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766387882.087598   27413 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766387882.087622   27413 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766387882.087625   27413 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766387882.087629   27413 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 07:18:02.092038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:__main__:Loading tokenizer: ProsusAI/finbert\n",
            "INFO:__main__:Preparing data (financial_phrasebank)...\n",
            "INFO:src.data_loader:Loading financial_phrasebank (sentences_allagree) from HuggingFace hub.\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1461: FutureWarning: The repository for financial_phrasebank contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/financial_phrasebank\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Map: 100% 1584/1584 [00:00<00:00, 6629.60 examples/s]\n",
            "Map: 100% 340/340 [00:00<00:00, 6181.73 examples/s]\n",
            "Map: 100% 340/340 [00:00<00:00, 7012.16 examples/s]\n",
            "INFO:src.data_loader:Prepared datasets with class weights: [2.4905660152435303, 0.5426515936851501, 1.3233082294464111]\n",
            "INFO:__main__:Detected 3 labels.\n",
            "INFO:__main__:Initializing model (LoRA=True)...\n",
            "INFO:src.model:Loading base model: ProsusAI/finbert\n",
            "INFO:src.model:Applying LoRA (PEFT) to the base model.\n",
            "trainable params: 444,675 || all params: 109,929,222 || trainable%: 0.4045\n",
            "INFO:__main__:=== Trainable parameters check ===\n",
            "INFO:__main__:Trainable param count: 74\n",
            "INFO:__main__:Trainable head params (classifier/score): ['base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias']\n",
            "INFO:__main__:Trainable LoRA params: 72\n",
            "INFO:__main__:Sample LoRA params: ['base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.default.weight']\n",
            "INFO:__main__:==================================\n",
            "/content/financial-nlp-lora-pipeline-main/src/trainer.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedCELossTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "INFO:src.trainer:Initialized WeightedCELossTrainer with class weights: tensor([2.4906, 0.5427, 1.3233])\n",
            "INFO:__main__:Starting training...\n",
            "2025/12/22 07:18:21 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/12/22 07:18:21 INFO mlflow.store.db.utils: Updating database tables\n",
            "2025/12/22 07:18:21 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/22 07:18:21 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2025/12/22 07:18:21 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/22 07:18:21 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "{'loss': 3.1567, 'grad_norm': 16.37203598022461, 'learning_rate': 9.8e-05, 'epoch': 0.51}\n",
            " 20% 98/495 [00:08<00:31, 12.44it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            " 18% 4/22 [00:00<00:00, 38.95it/s]\u001b[A\n",
            " 36% 8/22 [00:00<00:00, 31.23it/s]\u001b[A\n",
            " 55% 12/22 [00:00<00:00, 29.63it/s]\u001b[A\n",
            " 73% 16/22 [00:00<00:00, 29.58it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.27489686012268066, 'eval_accuracy': 0.9558823529411765, 'eval_macro_f1': 0.9467941005838316, 'eval_weighted_f1': 0.9565222257168181, 'eval_runtime': 0.7631, 'eval_samples_per_second': 445.551, 'eval_steps_per_second': 28.83, 'epoch': 1.0}\n",
            " 20% 99/495 [00:09<00:31, 12.44it/s]\n",
            "100% 22/22 [00:00<00:00, 29.64it/s]\u001b[A\n",
            "{'loss': 0.802, 'grad_norm': 3.324298143386841, 'learning_rate': 8.898876404494383e-05, 'epoch': 1.01}\n",
            "{'loss': 0.1969, 'grad_norm': 1.103873610496521, 'learning_rate': 7.775280898876405e-05, 'epoch': 1.52}\n",
            " 40% 197/495 [00:17<00:23, 12.44it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            " 23% 5/22 [00:00<00:00, 39.38it/s]\u001b[A\n",
            " 41% 9/22 [00:00<00:00, 34.02it/s]\u001b[A\n",
            " 59% 13/22 [00:00<00:00, 32.66it/s]\u001b[A\n",
            " 77% 17/22 [00:00<00:00, 32.16it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.08944948762655258, 'eval_accuracy': 0.9764705882352941, 'eval_macro_f1': 0.9681384005333894, 'eval_weighted_f1': 0.9766859174030702, 'eval_runtime': 0.7019, 'eval_samples_per_second': 484.401, 'eval_steps_per_second': 31.344, 'epoch': 2.0}\n",
            " 40% 198/495 [00:18<00:23, 12.44it/s]\n",
            "100% 22/22 [00:00<00:00, 31.76it/s]\u001b[A\n",
            "{'loss': 0.0806, 'grad_norm': 0.24165469408035278, 'learning_rate': 6.651685393258428e-05, 'epoch': 2.02}\n",
            "{'loss': 0.0796, 'grad_norm': 0.19300366938114166, 'learning_rate': 5.5280898876404495e-05, 'epoch': 2.53}\n",
            " 60% 297/495 [00:27<00:15, 12.41it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            " 23% 5/22 [00:00<00:00, 39.43it/s]\u001b[A\n",
            " 41% 9/22 [00:00<00:00, 34.35it/s]\u001b[A\n",
            " 59% 13/22 [00:00<00:00, 32.91it/s]\u001b[A\n",
            " 77% 17/22 [00:00<00:00, 32.22it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.08969226479530334, 'eval_accuracy': 0.9764705882352941, 'eval_macro_f1': 0.9709739458793267, 'eval_weighted_f1': 0.9766813654488009, 'eval_runtime': 0.6988, 'eval_samples_per_second': 486.547, 'eval_steps_per_second': 31.482, 'epoch': 3.0}\n",
            " 60% 297/495 [00:27<00:15, 12.41it/s]\n",
            "100% 22/22 [00:00<00:00, 31.88it/s]\u001b[A\n",
            "{'loss': 0.0962, 'grad_norm': 3.030660629272461, 'learning_rate': 4.404494382022472e-05, 'epoch': 3.03}\n",
            "{'loss': 0.0911, 'grad_norm': 0.16329310834407806, 'learning_rate': 3.2808988764044946e-05, 'epoch': 3.54}\n",
            " 80% 395/495 [00:36<00:08, 12.36it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            " 23% 5/22 [00:00<00:00, 38.13it/s]\u001b[A\n",
            " 41% 9/22 [00:00<00:00, 33.99it/s]\u001b[A\n",
            " 59% 13/22 [00:00<00:00, 32.59it/s]\u001b[A\n",
            " 77% 17/22 [00:00<00:00, 31.93it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.08407323807477951, 'eval_accuracy': 0.9764705882352941, 'eval_macro_f1': 0.9709739458793267, 'eval_weighted_f1': 0.9766813654488009, 'eval_runtime': 0.7175, 'eval_samples_per_second': 473.871, 'eval_steps_per_second': 30.662, 'epoch': 4.0}\n",
            " 80% 396/495 [00:37<00:08, 12.36it/s]\n",
            "100% 22/22 [00:00<00:00, 31.72it/s]\u001b[A\n",
            "{'loss': 0.0691, 'grad_norm': 0.18576636910438538, 'learning_rate': 2.157303370786517e-05, 'epoch': 4.04}\n",
            "{'loss': 0.0776, 'grad_norm': 0.06801565736532211, 'learning_rate': 1.0337078651685394e-05, 'epoch': 4.55}\n",
            "100% 495/495 [00:46<00:00, 11.23it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            " 18% 4/22 [00:00<00:00, 38.65it/s]\u001b[A\n",
            " 36% 8/22 [00:00<00:00, 31.55it/s]\u001b[A\n",
            " 55% 12/22 [00:00<00:00, 29.59it/s]\u001b[A\n",
            " 73% 16/22 [00:00<00:00, 28.90it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.08410266786813736, 'eval_accuracy': 0.9794117647058823, 'eval_macro_f1': 0.9736118401372639, 'eval_weighted_f1': 0.9795717617377637, 'eval_runtime': 0.767, 'eval_samples_per_second': 443.259, 'eval_steps_per_second': 28.681, 'epoch': 5.0}\n",
            "100% 495/495 [00:46<00:00, 11.23it/s]\n",
            "100% 22/22 [00:00<00:00, 28.97it/s]\u001b[A\n",
            "{'train_runtime': 48.0158, 'train_samples_per_second': 164.946, 'train_steps_per_second': 10.309, 'train_loss': 0.47432149781121147, 'epoch': 5.0}\n",
            "100% 495/495 [00:48<00:00, 10.21it/s]\n",
            "INFO:__main__:Evaluating on test set...\n",
            "100% 22/22 [00:00<00:00, 31.83it/s]\n",
            "100% 22/22 [00:00<00:00, 32.80it/s]\n",
            "INFO:__main__:Pred label distribution: Counter({1: 206, 2: 87, 0: 47})\n",
            "INFO:__main__:Test metrics: {'eval_loss': 0.07577338069677353, 'eval_accuracy': 0.9794117647058823, 'eval_macro_f1': 0.9736033405941846, 'eval_weighted_f1': 0.9795086484236225, 'eval_runtime': 0.7074, 'eval_samples_per_second': 480.619, 'eval_steps_per_second': 31.099, 'epoch': 5.0}\n",
            "INFO:__main__:Saving model to ./outputs\n"
          ]
        }
      ]
    }
  ]
}